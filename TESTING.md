# Testing Guide

Comprehensive testing guide for RaceCraft Live application.

## Backend Tests

### Setup

1. Install testing dependencies:
```bash
cd backend
pip install -r requirements.txt
```

2. Run all tests:
```bash
pytest
```

3. Run tests with coverage:
```bash
pytest --cov=models --cov=api --cov=services --cov-report=html
```

4. Run specific test files:
```bash
# ML model tests only
pytest tests/test_models.py -v

# API tests only
pytest tests/test_api.py -v
```

5. Run tests with markers:
```bash
# Run only unit tests
pytest -m unit

# Run only integration tests
pytest -m integration

# Skip slow tests
pytest -m "not slow"
```

### Test Coverage

Current test coverage includes:

**ML Models (`tests/test_models.py`):**
- ✅ PaceForecaster - Prediction accuracy, feature preparation, lap sequences
- ✅ ThreatDetector - Threat levels, pace comparison, defensive recommendations
- ✅ PitOptimizer - Window calculations, confidence scores, scenario simulation
- ✅ Integration tests - Full workflow with multiple models

**API Endpoints (`tests/test_api.py`):**
- ✅ Health checks - Root and health endpoints
- ✅ Pace forecast API - POST endpoint, validation, response structure
- ✅ Degradation analysis API - Analysis endpoint, stint health validation
- ✅ Threat detection API - Detection endpoint, gap analysis
- ✅ Pit window API - Recommendation and simulation endpoints
- ✅ Current status API - Pace, degradation, and overall status
- ✅ Error handling - Invalid endpoints, missing fields, validation
- ✅ Performance tests - Response times, concurrent requests

### Test Structure

```
backend/
├── tests/
│   ├── __init__.py
│   ├── test_models.py       # ML model unit tests
│   ├── test_api.py           # API integration tests
│   └── pytest.ini            # Pytest configuration
```

### Writing New Tests

Example test for a new model:

```python
import pytest
from models.your_model import YourModel

class TestYourModel:
    @pytest.fixture
    def model(self):
        return YourModel()

    def test_basic_functionality(self, model):
        result = model.predict(data)
        assert result is not None
        assert len(result) > 0
```

Example API test:

```python
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)

def test_new_endpoint():
    response = client.post("/api/your/endpoint", json={"data": "test"})
    assert response.status_code == 200
    assert "expected_field" in response.json()
```

## Frontend Tests

### Setup (Future)

Frontend testing will use:
- Jest for unit tests
- React Testing Library for component tests
- Cypress for E2E tests

```bash
cd frontend
npm test
```

## Performance Testing

### API Performance

Run performance tests:
```bash
pytest tests/test_api.py::TestPerformance -v
```

Expected benchmarks:
- Pace forecast: < 2 seconds
- Degradation analysis: < 1 second
- Threat detection: < 1.5 seconds
- Pit window optimization: < 2 seconds

### Load Testing

Use `locust` for load testing:

```bash
pip install locust

# Create locustfile.py
locust -f locustfile.py --host=http://localhost:8000
```

## Continuous Integration

### GitHub Actions Example

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt

    - name: Run tests
      run: |
        cd backend
        pytest --cov --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

## Test Data

### Sample Data Generation

Tests use synthetic data generated by models. To use real data:

1. Set `DATA_MODE=real` in `.env`
2. Place CSV files in `data/barber/`
3. Run tests with real data flag:
```bash
pytest --real-data
```

### Test Fixtures

Common test fixtures are defined in `conftest.py`:
- Sample lap data
- Vehicle telemetry
- Race configurations

## Debugging Tests

### Run with detailed output:
```bash
pytest -vv --tb=short
```

### Run specific test:
```bash
pytest tests/test_models.py::TestPaceForecaster::test_prediction_shape -v
```

### Drop into debugger on failure:
```bash
pytest --pdb
```

### Show print statements:
```bash
pytest -s
```

## Coverage Reports

After running tests with coverage, view the HTML report:

```bash
cd backend
pytest --cov --cov-report=html
open htmlcov/index.html  # macOS
xdg-open htmlcov/index.html  # Linux
```

Target coverage: **>80%** for all modules

## Common Issues

### ImportError

If you encounter import errors:
```bash
export PYTHONPATH="${PYTHONPATH}:/path/to/backend"
```

### Database/Cache Tests

Tests use in-memory cache by default. To test with Redis:
```bash
# Start Redis
docker run -d -p 6379:6379 redis

# Run tests
TEST_REDIS_URL=redis://localhost:6379 pytest
```

### Slow Tests

Mark slow tests:
```python
@pytest.mark.slow
def test_expensive_operation():
    # Long-running test
    pass
```

Skip during development:
```bash
pytest -m "not slow"
```

## Best Practices

1. **Isolation**: Each test should be independent
2. **Fast**: Unit tests should run in milliseconds
3. **Deterministic**: Tests should always produce same results
4. **Descriptive**: Test names should describe what they test
5. **Coverage**: Aim for >80% code coverage
6. **Fixtures**: Use fixtures for common test data
7. **Mocking**: Mock external dependencies (APIs, databases)

## Reporting Bugs

When reporting test failures, include:
- Test command used
- Full error output
- Python version
- Operating system
- Relevant environment variables

## Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [FastAPI Testing](https://fastapi.tiangolo.com/tutorial/testing/)
- [Coverage.py](https://coverage.readthedocs.io/)
